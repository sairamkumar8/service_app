
# User Management System â€“ FastAPI + PostgreSQL + ETL Pipeline

This project demonstrates a complete Data Engineering workflow implementing:

- REST API using **FastAPI**
- Relational database using **PostgreSQL (Docker)**
- SQL scripts for schema + sample data
- ETL pipeline using **Python + Pandas**
- Postman collection for API testing
- Clean folder structure and documentation

---

# ğŸš€ **1. Features Overview**

## **A. Backend API (FastAPI)**
Implements CRUD operations for:

- **Users**
- **Employment Info**
- **Bank Details**

### **API Endpoints**

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/users` | Create user + nested employment + bank info |
| GET | `/users` | Get all users (supports filters) |
| GET | `/users/{id}` | Get 1 user |
| PUT | `/users/{id}` | Update user |
| DELETE | `/users/{id}` | Delete user (cascade) |
| POST | `/users/{id}/employment` | Add extra employment record |
| POST | `/users/{id}/bank` | Add extra bank record |

Swagger UI:  
ğŸ‘‰ **http://127.0.0.1:8000/docs**

---

# ğŸ—„ï¸ **2. Database (PostgreSQL)**

Three tables are used:

```
users
employment_info
user_bank_info
```

SQL folder contains:

- `01_create_tables.sql`
- `02_insert_sample_data.sql`

These scripts can be executed in pgAdmin or psql.

---

# ğŸ”„ **3. ETL Pipeline**

Python script:  
ğŸ“„ `etl/group_users.py`

### **ETL Responsibilities:**

âœ” Connect to PostgreSQL  
âœ” Read all 3 tables  
âœ” Join and group users by:

- bank_name  
- company_name  
- pincode  

âœ” Generate CSVs with:

- `group_key`
- `user_count`
- `user_ids`

âœ” Save outputs in:

```
etl/output/
```

Example output files:

```
group_by_bank_<timestamp>.csv
group_by_company_<timestamp>.csv
group_by_pincode_<timestamp>.csv
```

---

# ğŸ§ª **4. Postman API Collection**

Postman collection is stored in:

```
postman/api_collection.json
```

Contains all API tests for:

- Create  
- Get  
- Filters  
- Update  
- Delete  
- Add bank & employment entries  

---

# ğŸ“ **5. Project Structure**

```
service_app/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â”œâ”€â”€ crud.py
â”‚   â”œâ”€â”€ database.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ group_users.py
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_create_tables.sql
â”‚   â””â”€â”€ 02_insert_sample_data.sql
â”‚
â”œâ”€â”€ postman/
â”‚   â””â”€â”€ api_collection.json
â”‚
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

# ğŸ”§ **6. Environment Variables (.env)**

Your `.env` file must contain:

```
DB_USER=postgres
DB_PASS=postgres
DB_HOST=localhost
DB_PORT=5432
DB_NAME=user_management
```

FastAPI + ETL script both read from these.

---

# â–¶ï¸ **7. How to Run the Backend**

```
cd service_app
.env\Scripts\Activate.ps1
python -m uvicorn main:app --reload --port 8000
```

Swagger:  
ğŸ‘‰ http://127.0.0.1:8000/docs

---

# â–¶ï¸ **8. How to Run ETL Script**

```
cd service_app
.env\Scripts\Activate.ps1
cd etl
python group_users.py
```

Outputs generated in:

```
etl/output/
```

---

# ğŸ“¦ **9. Technologies Used**

- FastAPI  
- PostgreSQL (Docker)  
- SQLAlchemy  
- Pandas  
- Python-dotenv  
- Postman  
- Uvicorn  
- VS Code  
- GitHub Desktop  

---

# ğŸ“ **10. Summary**

This project shows skills in:

- REST API development  
- Relational DB design  
- Writing SQL  
- ETL development using Python & Pandas  
- Using Docker  
- API testing with Postman  
- Logging, query filters, cascade delete  
- Organizing a production-style project  

---

# âœ… **Project Ready for Submission**

This codebase is clean, modular, tested and well-documented â€” suitable for Data Engineering assessment and TL evaluation.
